<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>Tiffany Chen | Work</title>
        <meta name="description" content="Tiffany Chen Portfolio">
        <meta name="author" content="Tiffany Chen">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="../css/styles.css">
        <script
          src="http://code.jquery.com/jquery-3.3.1.slim.min.js"
          integrity="sha256-3edrmyuQ0w65f8gfBsqowzjJe2iM6n0nKciPUp8y+7E="
          crossorigin="anonymous"></script>
    </head>
    <body>
        <div id="pre-loader"></div>   
        <div id="content-wrapper">
            <nav id="main-nav">
                <div><a href="../index.html"><img src="../img/logo.svg" id="nav-logo"/></a></div>
                <div id="right-nav">
                    <a href="../work.html">work&mdash;</a>
                    <a rel="prerender" href="../about.html">about&mdash;</a>
                </div>
            </nav>
            <div id="page-content">
                <div class="project-page">
                    <div class="page-grid">
                        <div id="page-nav">
                            <a href="../work.html">
                                <img src="../img/icons/back.svg"/>
                            </a>
                            <a id="page-menu">
                                <img src="../img/icons/menu.svg" id="menu-icon"/>
                            </a>
                        </div>
                        <div id="project-header">
                            <img src="../img/projects/vr_for_robot_teleop/cover_photo.gif"/>
                            <div id="project-title">
                                <h3>VR for Robot Teleoperation</h3>
                                <h6>Using VR for more intuitive way to interface with robots</h6>
                            </div>
                        </div>
                        <div class="main-content">
                             <div class="small-grid">
                                <div>
                                    <div class="section-title">team</div>
                                    <div class="section-content">
                                        Marisa Lu<br/>
                                        Lucas Ochoa<br/>
                                        Annie Huang<br/>
                                        Mary Safy<br/>
                                        Riva Fouzdar
                                    </div>
                                </div>
                                <div>
                                    <div class="section-title">timeframe</div>
                                    <div class="section-content">
                                        Jan – May 2019
                                    </div>
                                </div>
                                 <div>
                                    <div class="section-title">role</div>
                                    <div class="section-content">
                                        Technical Lead <br/>
                                        Design
                                    </div>
                                </div>
                            </div>
                            <h2 class="main-subtitle" id="section-1">
                                project overview
                            </h2>
                            <div class="section-wrapper">
                                <p class="main-paragraph">
                                    The notion of robotic personal assistants has long captured our collective imagination, from the mechanical automata of ancient China to the cyborg tin woodmen and intelligent sidekicks of pop culture. The hardware is already here, so where are these vaunted robots of science fiction? 
                                </p>
                                <p class="main-paragraph">
                                    Our client was the Robots Perceiving and Doing Lab at Carnegie Mellon University. Their goal is to use machine learning to create these personal robotic assistants. The goal of the project was to help the lab build a usable, intuitive virtual reality (VR) environment to faciliate the collection of user demonstrations of tasks. Once the data is collected, machine learning techniques use the demonstrations to teach robots to complete tasks. This data and method of robot teaching can allow for remote instruction, or teleoperation, for tasks that cannot be done in person.
                                </p>
                            </div>
                            <h2 class="main-subtitle" id="section-2">
                                primary questions
                            </h2>
                            <div class="section-wrapper">
                                 <ul>
                                    <li>Is a moderator necessary for the collection of task data? How will errors, time, task completion be tracked in the environment? Should a researcher be moderating task collection or should users self-moderated?</li>
                                    <li>How would the user interact with the interface and the environment? How can we make the environment easy to navigate and easy to complete precise actions?</li>
                                    <li>What is the primary user perspective while operating the robot? Should the VR interface happen in first person, third person, or both?</li>
                                    <li>How do we account for latency between real time robot movement and user input?</li>
                                </ul>
                                <img src="../img/projects/vr_for_robot_teleop/questions.png"/>
                            </div>
                            <h2 class="main-subtitle" id="section-3">
                                solution
                            </h2>
                            <div class="section-wrapper">
                                <p class="main-paragraph">
                                    Our solution consists of 3 parts, with each part addressing a different aspect. There are two major applications: <span class="bolded">the VR environment with its spatial interfaces and control mechanisms </span> and <span class="bolded"> a 2D interface for the researcher moderating the task collection from the VR user.</span> We also created <span class="bolded">a final proof of concept video to showcase the VR interface in its future state.</span>
                                </p>
                                <p class="main-paragraph">
                                Our solution addressed these problems by:</p>
                                <ul>
                                    <li><div class="bolded">Moderated vs. Unmoderated</div>Had the VR interface moderated by a researcher who would log errors, send messages, and track time. 
                                    </li>
                                    <li><div class="bolded">Augmenting Precision</div>Designed a cursor (ghost end effector) and cursor marker (ghost trail) to specific points within the VR environment for precise and accurate navigation.
                                    </li>
                                    <li><div class="bolded">First vs. Third Person</div>Combined first and third person to take advantage of the benefits of both perspectives. 
                                    </li>
                                    <li><div class="bolded">Path Planning vs. Live Control</div>Disguised lag of real-time robot movement from user’s movement in VR environment.
                                    </li>
                                </ul>
                            </div>
                             <h2 class="main-subtitle" id="section-4">
                                prototype
                            </h2>
                            <div class="section-wrapper">
                                <p class="main-paragraph">
                                    The prototype (done in Unity) is the actual working demo that the team created to document and test novel interactions for VR teleoperation. In the video, the user's task to try to teach the robot to pick up the fruit and put it in the bowl.
                                </p>
                                <iframe src="https://player.vimeo.com/video/385088844" width="100%" height="500px" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>
                            
                            </div>
                            <h2 class="main-subtitle" id="section-5">
                                the future
                            </h2>
                            <div class="section-wrapper">
                                <p class="main-paragraph">
                                    The below video is an additional exploration of what the future of VR teleoperation might look like. The team explored for the future of VR for robot teleoperation.
                                </p>
                                <iframe src="https://player.vimeo.com/video/385087299" width="100%" height="500px" align="middle" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>
                            
                            </div>
                            <h2 class="main-subtitle" id="section-6">
                                design process
                            </h2>
                            <div class="section-wrapper">
                                <h3 class="sub-subtitle">user studies</h3>
                                <p class="main-paragraph">
                                    Before jumping into designing VR, we had to first understand the design patterns and pitfalls of VR. To help us answer this question, we conducted exploratory think-aloud usability tests with 11 participants, who all had little or no experience using VR. These participants went through think-aloud exercises with 3 existing VR programs — Microsoft Maquette, Virtual Virtual Reality, and Google Earth VR. With this study, we aimed to answer the following questions:
                                </p>
                                <ul>
                                    <li>What are the pain points and points of confusion for a user in a VR environment given instructions and a series of small tasks? </li>
                                    <li>What design patterns are commonly being used in popular VR applications? (for menus, cursors, controls, etc.)</li>
                                </ul>
                                <img src="../img/projects/vr_for_robot_teleop/journey_map.png"/>
                                <p class="main-paragraph">
                                    Moving forward from these studies, we kept these four things in mind:
                                </p>
                                <ul>
                                    <li>Use clear forward feedback and instructions to allow for easier onboarding.</li>
                                    <li>Current VR UIs are lack in discoverability and providing meaningful feedback and guidance.</li>
                                    <li>2D UI paradigms do not necessarily mix with 3D UI paradigms.</li>
                                    <li>Physical limitations such as space must be kept in mind.</li>
                                </ul>
                                <h3 class="sub-subtitle">storyboards</h3>
                                <p class="main-paragraph">
                                    We created multiple storyboards to define potential solutions to the various usability problems faced in VR environments. The storyboard details how a user would enter the VR environment, view a tutorial for a cup stacking task, and then attempt to complete that task. Below is an aggregated storyboard detailing important interactions that were discussed.
                                </p>
                                <img src="../img/projects/vr_for_robot_teleop/storyboard.png"/>
                                <h3 class="sub-subtitle">3d prototyping</h3>
                                <p class="main-paragraph">
                                    To get a better understanding of what were the key interactions, the team did some whiteboxing in Microsoft Maquette. In this process, we iterated on different controller interactions and environment setups.
                                </p>
                                <div class="img-grid-2">
                                    <div><img class="same-height" src="../img/projects/vr_for_robot_teleop/maquette4.png"/></div>
                                    <div><img  class="same-height" src="../img/projects/vr_for_robot_teleop/maquette6.png"/></div>
                                    <div><img class="same-height" src="../img/projects/vr_for_robot_teleop/maquette2.png"/></div>
                                    <div><img  class="same-height" src="../img/projects/vr_for_robot_teleop/maquette1.gif"/></div>
                                </div>
                            </div>
                            <h2 class="main-subtitle" id="section-7">
                                defining terms
                            </h2>
                            <div class="section-wrapper">
                                <p class="main-paragraph">
                                    Taking our storyboards and prototypes, we conducted speed dating session with 6 users. Through these sessions, we sought to inform ourselves on key factors. After gathering the feedback, the team discussed and defined key terms and approaches.
                                </p>
                                <div class="sub-subtitle">1st vs. 3rd person</div>
                                <div class="img-grid-2 align-items-center">
                                    <div> 
                                        <img src="../img/projects/vr_for_robot_teleop/first_person.gif">
                                    </div>
                                    <div>
                                        <p class="main-paragraph">
                                            <span class="font-bold">First Person</span>
                                            <br/>
                                            First person perspective is more intuitive, since it more closely mimics real life. However, first person lends itself to unpredictable inverse kineatics and physics when the end effector is tied directly to user motion.  
                                        </p>
                                    </div>
                                </div>
                                <div class="img-grid-2 align-items-center">
                                    <div>
                                        <p class="main-paragraph">
                                            <span class="font-bold">Third Person</span>
                                            <br/>
                                            Third person more closely mimics current kinesthetic learning and is what many researchers already employ with robot training. 
                                        </p>
                                    </div>
                                    <div> 
                                        <img src="../img/projects/vr_for_robot_teleop/third_person.gif">
                                    </div>
                                </div>
                                <div class="sub-subtitle">path planning vs. live control</div>
                                <p class="main-paragraph">
                                    <span class="font-bold">Path Planning</span>
                                    <br/>
                                    Path planning allows the user to set positions for the robot to travel to. This method of robot control accounts for the lag between the user’s action completed in the VR environment and real-time robot movement.
                                </p>
                                <p class="main-paragraph">
                                    <span class="font-bold">Live Control</span>
                                    <br/>
                                    With live control, the robots does exactly as the user does in real time. Live control is more intuitive for the user to complete tasks.
                                </p>
                                <p class="main-paragraph">
                                    <span class="font-bold">Moving Forward</span>
                                    <br/>
                                    Based on discussions, we decided to move foward with a combination of first and third person to use the advantages of both perspectives. Along with this, we decided to use path planning to help account for system lag. 
                                </p>
                                
                            </div>
                            
                            <h2 class="main-subtitle" id="section-8">
                                mid-fidelity interactions
                            </h2>
                            <div class="section-wrapper">
                                <p class="main-paragraph">
                                    At this stage of the project, we focused on identifying ways that users can better control the robot to complete tasks.
                                </p>
                                <div class="sub-subtitle">augmenting precision</div>
                                    <p class="main-paragraph">
                                        In these explorations, we looked at the "Key-Ignition" model as a physical signifier of how to engage with the robot's end effector. We also did further explorations on proximity based activations. 
                                    </p>
                                </div>
                                <div class="img-grid-2">
                                    <div><img  class="same-height" src="../img/projects/vr_for_robot_teleop/maquette1.gif"/></div>
                                    <div><img  class="same-height" src="../img/projects/vr_for_robot_teleop/maquette3.png"/></div>
                                    <div><img  class="same-height" src="../img/projects/vr_for_robot_teleop/maquette7.png"/></div>
                                    <div><img  class="same-height" src="../img/projects/vr_for_robot_teleop/maquette8.png"/></div>
                                </div>
                            
                        
                            <h2 class="main-subtitle" id="section-9">
                                control hierarchy
                            </h2>
                            <div class="section-wrapper">
                                <div class="sub-subtitle">assumptions</div>
                                <p class="main-paragraph">
                                    We made a few assumptions in the process of designing the controls.
                                </p>
                                <ul>
                                    <li>Robot manipulation is one handed and primarily in first person, so dominant hand primarily focuses on this task.</li>
                                    <li>Separating controls based on dominant and non-dominant controllers helps users distinguish between physically controlling the robot and changing the mode that the user is operating in.</li>
                                    <li>Separation of controls allows for fluidity of movement.</li>
                                </ul>
                                <img src="../img/projects/vr_for_robot_teleop/controls.png"/>
                                <div class="sub-subtitle">trajectory-based control vs. position-based control</div>
                                <p class="main-paragraph">
                                    In early explorations, we focused on position-based control, where the robot moves it previous position to the next planned position. This method of manipulation does not allow for smooth, continuous actions and looses the fluidity of the motion. Thus, we pivoted to designing controls with trajectory-based control of the end effector.
                                </p>
                                <img src="../img/projects/vr_for_robot_teleop/trajectory_control.png"/>
                            </div>
                            <h2 class="main-subtitle" id="section-10">
                                final design 
                            </h2>
                            <div class="section-wrapper">
                                <div class="sub-subtitle">fluid 1st and 3rd</div>
                                <p class="main-paragraph">
                                    <span class="font-bold">First Person</span>
                                    <br/>
                                    The user can stand in the position of the robot and manipulate it.
                                </p>
                                <p class="main-paragraph">
                                    <span class="font-bold">Third Person</span>
                                    <br/>
                                    Alternatively, the user can move around the robot and puppeteer it. 
                                </p>
                                <p class="main-paragraph">
                                    In user tests, users primarily prefer to stand in first person and would occasionally move to other angles to manipulate the robot.
                                </p>
                                <div class="sub-subtitle">environment</div>
                                <p class="main-paragraph">
                                    <img class="phone-img" src="../img/projects/vr_for_robot_teleop/environment.PNG"/>
                                    The environment was designed to allow for a variety of tasks. The user is placed into relatively empty environment and presented with a task. The task is demonstrated and the user is then expected to replicated it.
                                </p>
                                <div class="sub-subtitle">controls</div>
                                <p class="main-paragraph">
                                    As previously mentioned, we decided to split up dominant controller and non-dominant controller actions. The actions of the dominant controller are primarily focused on the <span class="font-bold">movements of the robot arm.</span>
                                </p>
                                <div class="img-grid-2 align-items-center">
                                    <div> 
                                        <img src="../img/projects/vr_for_robot_teleop/create_trajectory.gif">
                                    </div>
                                    <div>
                                        <p class="main-paragraph">
                                            <span class="font-bold">Dominant Controller – Creating a Trajectory</span>
                                            <br/>
                                            The user presses and holds the trackpad to "draw" a trajectory. This leaves a "ghost" trail or a series of positions for robot to mimic. 
                                        </p>
                                    </div>
                                </div>
                                <div class="img-grid-2 align-items-center">
                                    <div>
                                        <p class="main-paragraph">
                                            <span class="font-bold">Dominant Controller – Closing the End Effector</span>
                                            <br/>
                                            To control the "claw" or end effector, the user presses down on the trigger and the trackpad at the same time, simulating a gripping motion. The color of the ghost changes to signify a closed state.
                                        </p>
                                    </div>
                                    <div> 
                                        <img src="../img/projects/vr_for_robot_teleop/endeffector.gif">
                                    </div>
                                </div>
                                <div class="img-grid-2 align-items-center">
                                    <div> 
                                        <img src="../img/projects/vr_for_robot_teleop/nondom_controller.gif">
                                    </div>
                                    <div>
                                        <p class="main-paragraph">
                                            <span class="font-bold">Non-dominant Controller – All Other Actions</span>
                                            <br/>
                                            The non-dominant controller contains all other functions. <br/>
                                            Reset – Resets scene<br/>
                                            Undo – Removes most recent section of ghost trail<br/>
                                            Redo – Brings back most recent section of ghost trail<br/>
                                            Run – Sends ghost trail to the robot to execute
                                        </p>
                                    </div>
                                </div>
                            </div>
                            <h2 class="main-subtitle" id="section-11">
                                moderated interface
                            </h2>
                            <div class="section-wrapper">
                                <div class="sub-subtitle">moderated vs. unmoderated interface</div>
                                <p class="main-paragraph">
                                    <span class="font-bold">Moderated</span>
                                    <br/>
                                    When the interface is moderated by a researcher, the researcher can more accurately monitor errors, instead of depending on the user to track their own errors. The user can simply focus on completing the task in front of them, without being overwhelmed with this secondary task.
                                </p>
                                <p class="main-paragraph">
                                    <span class="font-bold">Unmoderated</span>
                                    <br/>
                                    When the interface is unmoderated, less manpower is required and tasks can more easily be completed remotely. Instead machine learning can be used identify and correct mistakes.
                                </p>
                                <div class="sub-subtitle">final interface</div>
                                <div class="img-grid-2 align-items-center">
                                    <div> 
                                        <img src="../img/projects/vr_for_robot_teleop/Mockup1-3085.png">
                                    </div>
                                    <div>
                                        <p class="main-paragraph">
                                            <span class="font-bold">Tasks</span>
                                            <br/>
                                            The task screen allows the moderator to view testing sessions as they happen and also view data collected from previous testing sessions.
                                        </p>
                                    </div>
                                </div>
                                <div class="img-grid-2 align-items-center">
                                    <div>
                                        <p class="main-paragraph">
                                            <span class="font-bold">Moderator View</span>
                                            <br/>
                                            The moderator will have concurrent feeds of the physical robot in real time, the VR user in their physical environment, as well as a livestream of what the VR user is seeing in the virtual space.
                                        </p>
                                    </div>
                                    <div> 
                                        <img src="../img/projects/vr_for_robot_teleop/Mockup3-3122.png">
                                    </div>
                                    
                                </div>
                                <div class="img-grid-2 align-items-center">
                                    <div> 
                                        <img src="../img/projects/vr_for_robot_teleop/Mockup4-3128.png">
                                    </div>
                                    <div>
                                        <p class="main-paragraph">
                                            <span class="font-bold">Error Tracking</span>
                                            <br/>
                                            Researchers can input non-fatal and fatal errors manually, by pressing a button on the upper right hand menu. Users are informed through the VR interface that they have made an error.
                                        </p>
                                    </div>
                                </div>
                                <div class="img-grid-2 align-items-center">
                                    <div>
                                        <p class="main-paragraph">
                                            <span class="font-bold">Notifications</span>
                                            <br/>
                                            Researchers can send messages to the user while they are in the VR environment to provide hints or instructions. They can also create internal notes during the user test, which are not seen by the user.
                                        </p>
                                    </div>
                                    <div> 
                                        <img src="../img/projects/vr_for_robot_teleop/Mockup5-3126.png">
                                    </div>
                                    
                                </div>
                                <div class="img-grid-2 align-items-center">
                                    <div> 
                                        <img src="../img/projects/vr_for_robot_teleop/Mockup6-3124.png">
                                    </div>
                                    <div>
                                        <p class="main-paragraph">
                                            <span class="font-bold">Task Completion</span>
                                            <br/>
                                            When the user sufficiently completes the task, the moderator marks the task as complete, sending the user a message showing their completion time. The moderator can then view aggregate data or return to the task screen. 
                                        </p>
                                    </div>
                                    
                                </div>
                                <div class="img-grid-2 align-items-center">
                                    <div>
                                        <p class="main-paragraph">
                                            <span class="font-bold">Data Aggregation</span>
                                            <br/>
                                            Researchers can view aggregate data of all users who have completed tasks. They can also view individual data and recordings from each user test and add time markers, errors, and other notes after the test has taken place.
                                        </p>
                                    </div>
                                    <div> 
                                        <img src="../img/projects/vr_for_robot_teleop/Mockup7-3119.png">
                                    </div>
                                    
                                    
                                </div>
                            </div>
                            <h2 class="main-subtitle" id="section-12">
                                reflection
                            </h2>
                            <div class="section-wrapper">
                                <p class="main-paragraph">
                                    Our goal was to design a more intuitive interface for instructing robots to complete a task. The hope is that we can use this data to better enable robot intelligence and provide our client with the data necessary to train these robots. By bridging the gap between physical motion and robot operation, it opens a world of opportunities for controlling devices remotely, accurately, and precisely.
                                </p>
                                <p class="main-paragraph">There are exciting discoveries yet to made in this realm and our contributions have just scratched the surface of the future of robot teleoperation. While our role in this project is over, this project is still ongoing the CMU Robots Perceiving and Doing Lab.


                                </p>
                            </div>
                           
                    </div>
                    </div>
                </div>
            </div>
            <div id="footer">
                <div id="copyright">
                    copyright &copy; tiffany chen 2021<br/>
                    all rights reserved.
                </div>
                <div>
                    <div id="social-media-links">
                        <a href="https://github.com/tiffanyhchen">github</a>
                        <a href="https://www.linkedin.com/in/tiffanychen037/">linkedin</a>
                        <a href="mailto:contact@tiffany-chen.com">email</a>
                    </div>
                </div>
            </div>
        </div>
        <script src="../js/scrollnav.min.umd.js"></script>
        <script src="../js/project.js"></script>
        <script>
            const content = document.querySelector('.main-content');

            scrollnav.init(content, { 
              debug: false
            });
        </script>
        <script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@12.4.0/dist/lazyload.min.js"></script>
    </body>
</html>
